{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNtktS6smTmahDmDbe91I9u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/idowujames/-Kenya-Clinical-Reasoning-Challenge/blob/main/Kenya_Clinical_Reasoning_Challenge_Training(BioGPT).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# We begin to train our model, starting with by fine-tunning using BioGPT"
      ],
      "metadata": {
        "id": "gmG80xfU3Ye8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install sacremoses for BioGPT\n",
        "!pip install sacremoses --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_66y_vx9v2j",
        "outputId": "4a48f40d-ca20-4463-9acd-0b485dcc2364"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/897.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m686.1/897.5 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m890.9/897.5 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DTHPJS1o3TDd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    AutoModelForCausalLM\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the dataset"
      ],
      "metadata": {
        "id": "w0NhYaku5nA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('cleaned_dataset.csv')\n",
        "\n",
        "print(df.shape)\n",
        "dataset = Dataset.from_pandas(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwFWa6D749vp",
        "outputId": "fe3c9156-64a9-4a36-b5f2-a6e136462d75"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(386, 14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset splitting"
      ],
      "metadata": {
        "id": "23NA-G4B6EPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using 90% for training and 10% for validation\n",
        "train, val = dataset.train_test_split(test_size=0.1, seed=42).values()\n",
        "\n",
        "print(train.shape)\n",
        "print(val.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UTilWbk56lM",
        "outputId": "1055c83f-c14d-4761-c606-68b4716217e7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(347, 14)\n",
            "(39, 14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization"
      ],
      "metadata": {
        "id": "pY2UC9V58A3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'microsoft/biogpt'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "  tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "YIe472NW6lD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a tokenizer function\n",
        "def tokenize_function(examples):\n",
        "    # For each example, we combine the prompt and clinician response.\n",
        "    # We use the tokenizer's end-of-sentence token as a separator and at the very end.\n",
        "    # This teaches the model the full structure of a prompt-response pair.\n",
        "    text_with_format = [prompt + tokenizer.eos_token + response + tokenizer.eos_token\n",
        "                        for prompt, response in zip(examples['Prompt'], examples['Clinician'])]\n",
        "\n",
        "\n",
        "    return tokenizer(text_with_format, truncation=True, max_length=256)\n",
        "\n",
        "\n",
        "# Appling the function to the entire dataset.\n",
        "tokenized_train_dataset = train.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=train.column_names\n",
        ")\n",
        "\n",
        "tokenized_test_dataset = val.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=val.column_names\n",
        ")\n",
        "\n",
        "print(\"Tokenization Complete\")\n",
        "print(tokenized_train_dataset[0])"
      ],
      "metadata": {
        "id": "Ee46Ec099kge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training our model"
      ],
      "metadata": {
        "id": "4EDQknOMAE5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-Tuning the BioGPT Model with our data\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    warmup_steps=50,\n",
        "    weight_decay=0.01,               # Regularization to prevent overfitting\n",
        "    logging_steps=10,\n",
        "\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=50,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=50,\n",
        "    load_best_model_at_end=True,\n",
        "\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "# The data collator is a helper function that takes the tokenized samples and\n",
        "# batches them together, handling padding dynamically.\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "# Initializing the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_test_dataset,\n",
        "    data_collator=data_collator,         # The dynamic padding helper\n",
        ")\n",
        "\n",
        "# Starting the Fine-Tuning\n",
        "print(\"--- Starting Fine-Tuning ---\")\n",
        "trainer.train()\n",
        "\n",
        "# 6. Save the fine-tuned model\n",
        "final_model_path = \"./final_biogpt_model\"\n",
        "trainer.save_model(final_model_path)\n",
        "tokenizer.save_pretrained(final_model_path)\n",
        "print(f\"Final model saved to {final_model_path}\")"
      ],
      "metadata": {
        "id": "idjSaByt-nsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compressing and Downloading the Fine-tuned model As ZIP"
      ],
      "metadata": {
        "id": "8xW-G2CG4wTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "def download_model_as_zip():\n",
        "\n",
        "    model_folder = \"./final_biogpt_model\"\n",
        "    zip_filename = \"biogpt_model.zip\"\n",
        "\n",
        "    # Create zip file\n",
        "    print(f\"Compressing {model_folder} into {zip_filename}\")\n",
        "    shutil.make_archive(\"biogpt_model\", 'zip', model_folder)\n",
        "\n",
        "    # Checking file size\n",
        "    zip_size = os.path.getsize(zip_filename) / (1024 * 1024)\n",
        "    print(f\"Zip file size: {zip_size:.2f} MB\")\n",
        "\n",
        "    # Downloading the zip file\n",
        "    print(\"Starting download\")\n",
        "    files.download(zip_filename)\n",
        "    print(\"Download complete! Check your Downloads folder.\")\n",
        "\n",
        "# Run this method\n",
        "download_model_as_zip()"
      ],
      "metadata": {
        "id": "a1ogEEVK4qR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Runing Inference On The Test Data"
      ],
      "metadata": {
        "id": "Wr41xr9g5Tkv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "final_model_path = \"./final_biogpt_model\"\n",
        "\n",
        "# Loading the fine-tuned model\n",
        "model = AutoModelForCausalLM.from_pretrained(final_model_path).to('cuda')\n",
        "\n",
        "# Loading the tokenizer of our fine-tuned model.\n",
        "tokenizer = AutoTokenizer.from_pretrained(final_model_path)"
      ],
      "metadata": {
        "id": "_EWs0sX03YJ_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the test data\n",
        "test_df = pd.read_csv('test.csv')\n",
        "test_df.head()"
      ],
      "metadata": {
        "id": "Z25nBklt3XqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Starting the inference"
      ],
      "metadata": {
        "id": "HFDriCFm5yWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generated_responses = []\n",
        "\n",
        "print(\"Starting Inference on Test Data\")\n",
        "\n",
        "# Loop through each row in the test DataFrame\n",
        "for index, row in test_df.iterrows():\n",
        "    prompt_text = row['Prompt']\n",
        "\n",
        "    # formating the input just like the training,\n",
        "    # The eos_token signals to the model where the prompt ends and generation should begin.\n",
        "    formatted_prompt = prompt_text + tokenizer.eos_token\n",
        "\n",
        "    # Tokenizing the formatted prompt and move it to the GPU\n",
        "    inputs = tokenizer(formatted_prompt, return_tensors='pt').to('cuda')\n",
        "\n",
        "    # Generate the output from the model\n",
        "    output_sequences = model.generate(\n",
        "        input_ids=inputs['input_ids'],\n",
        "        attention_mask=inputs['attention_mask'],\n",
        "        max_length=256,  # Max length for the generated response\n",
        "        num_beams=5,\n",
        "        no_repeat_ngram_size=2,\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "    # Decoding the generated token IDs back into text\n",
        "    generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
        "\n",
        "    # --- Post-processing: Cleaning the output ---\n",
        "    # The generated text will contain our original prompt. We need to remove it.\n",
        "    # We find the length of the original prompt and take the substring that comes after it.\n",
        "    final_response = generated_text[len(prompt_text):].strip()\n",
        "\n",
        "    # Add the cleaned response to our list\n",
        "    generated_responses.append(final_response)\n",
        "\n",
        "    # Print progress every 10 prompts\n",
        "    if (index + 1) % 10 == 0:\n",
        "        print(f\"Generated response for {index + 1}/{len(test_df)} prompts.\")\n",
        "\n",
        "print(\"Inference Complete\")"
      ],
      "metadata": {
        "id": "v5prXCNe3XU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding the generated text to the test dataframe to see how well the text\n",
        "# accurately answers the prompts\n",
        "test_df['Generated Response'] = generated_responses\n",
        "\n",
        "test_df.sample(5)"
      ],
      "metadata": {
        "id": "_fOyw-2I3Wvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### From the outputs, it appears the fine-tuned model is prone to hallucination. The model's underlying medical knowledge learned from our small dataset is not robust enough, causing it to generate plausible-sounding but incorrect information.\n",
        "\n",
        "### Next step after a few hyper-parameter tuning is to try fine-tuning another base model that would work better for our datase\n"
      ],
      "metadata": {
        "id": "mjby-0WZ7iZC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eLhqBLOP8CFe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}